# -*- coding: utf-8 -*-
"""analisis-sentimen-tiktok.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g88ZwwyaVH3s4TiYRndpmjwnfa49c5j4
"""

!pip install Sastrawi
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime as dt
import re
import string

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk import download as nltk_download
nltk_download('punkt')
nltk_download('stopwords')
nltk_download('wordnet')

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

from wordcloud import WordCloud

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score, precision_score
from sklearn.preprocessing import LabelEncoder

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D, Conv1D, GlobalMaxPooling1D, Bidirectional, MaxPooling1D, Flatten, GRU, SimpleRNN
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, Callback
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam

import requests
import csv
from io import StringIO
import tensorflow as tf

df = pd.read_csv('data_scraping_tiktok.csv')

df.shape

df.head()

# Convert the 'at' column to datetime
df['at'] = pd.to_datetime(df['at'])

# Filter 2024-2025
df_2024_2025 = df[(df['at'].dt.year == 2024) | (df['at'].dt.year == 2025)]

# Eksplorasi
print(f"Jumlah data 2024-2025: {len(df_2024_2025)}")
print("\nDistribusi rating:")
print(df_2024_2025['score'].value_counts())
print("\nMissing values:")
print(df_2024_2025.isnull().sum())
print("\nSampel teks:")
print(df_2024_2025['content'].head(10))

# Drop specified columns
df_2024_2025 = df_2024_2025.drop(columns=['thumbsUpCount', 'reviewCreatedVersion', 'replyContent', 'repliedAt', 'appVersion', 'reviewId','userName', 'userImage']) # Only drop the existing columns

df = df_2024_2025

df.head()

# 1. Cek info awal
print("Jumlah baris dan kolom:", df.shape)
print("\nInfo dataframe:")
print(df.info())
print("\nJumlah missing value per kolom:")
print(df.isnull().sum())

df['content'].head(5)

# 7. (Opsional) Cek distribusi skor sebagai dasar labeling
print("\nDistribusi skor:")
print(df['score'].value_counts())

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from collections import Counter

# Download the necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab') # Download the punkt_tab resource

# 1. Melihat isi kolom `content`
print("Contoh isi kolom 'content':")
print(df['content'].head())  # Menampilkan 5 baris pertama

# 2. Mencari kata yang paling banyak keluar
def preprocess_text(text):
    """Melakukan preprocessing teks."""
    text = text.lower()  # Case folding
    tokens = word_tokenize(text)  # Tokenization
    stop_words = set(stopwords.words('english'))  # Stop word removal
    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatization
    return tokens

# Mengumpulkan semua kata dari kolom 'content'
all_words = []
for text in df['content']:
    all_words.extend(preprocess_text(text))

# Menghitung frekuensi kemunculan setiap kata
word_freq = Counter(all_words)

# Menampilkan 10 kata yang paling sering muncul
print("\n20 kata yang paling sering muncul:")
print(word_freq.most_common(20))

import re
import string
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

# Unduh stopwords dan tokenizer
nltk.download('punkt')
nltk.download('stopwords')

# 1. Cleaning text
def cleaningText(text):
    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Hapus mentions
    text = re.sub(r'#[A-Za-z0-9]+', '', text)  # Hapus hashtags
    text = re.sub(r'RT[\s]+', '', text)  # Hapus RT
    text = re.sub(r"http\S+", '', text)  # Hapus linkstext = re.sub(r'[^\w\s]', '', text)  # Hapus tanda baca
    text = re.sub(r'[0-9]+', '', text)  # Hapus numbers
    text = re.sub(r'[^\w\s]', '', text)  # Hapus tanda baca
    text = text.replace('\n', ' ')  # Ganti baris baru dengan spasi
    text = text.translate(str.maketrans('', '', string.punctuation))  # Hapus tanda baca lagi
    text = text.strip()  # Hapus spasi di awal dan akhir
    return text

# 2. Case folding
def casefoldingText(text):
    return text.lower()

# 3. Tokenizing
def tokenizingText(text):
    return word_tokenize(text)

def filteringText(text):
    list_stopwords = set(stopwords.words('indonesian'))
    list_stopwords_en = set(stopwords.words('english'))
    list_stopwords.update(list_stopwords_en)

    additional_stopwords = [
        'ini', 'saya', 'nya', 'si', 'aja', 'ajah', 'cuma', 'kok', 'tuh', 'doang',
        'dah', 'eh', 'ko', 'lah', 'deh', 'ya', 'nih', 'gitu', 'kmn', 'tpi',
        'gue', 'aku', 'ku', 'dong', 'dlu', 'lgi', 'banget', 'sama', 'buat', 'kalo',
        'mau', 'pas', 'udah', 'terus', 'tadi', 'skrng',
        'admin', 'adminnya', 'min', 'minnya', 'sobat', 'kakak', 'bro', 'sis',
        'guys', 'gaes', 'gaess', 'om', 'tante',
        'woy', 'woi', 'hai', 'cuy', 'lo', 'loh', 'plis', 'woe', 'hallo', 'euy',
        'anjir', 'njir', 'sih', 'tau', 'lho', 'btw', 'cmn', 'yaa', 'yaaa', 'yh',
        'gmn', 'gmana','gmna'
        ]


    list_stopwords.update(additional_stopwords)

    return [word for word in text if word not in list_stopwords]

# 5. Stemming (Opsional, tidak dipakai default)
def stemmingText(text):
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    return ' '.join([stemmer.stem(word) for word in text.split()])

# 6. To sentence
def toSentence(list_words):
    return ' '.join(list_words)

# Gabungkan semua tweet jadi satu string besar
all_text = ' '.join(df['content'].astype(str).apply(cleaningText))

# Tokenisasi manual (split by space)
all_words = all_text.split()

# Hitung frekuensi kata
word_counts = Counter(all_words)

# Ambil kata unik (yang muncul hanya sekali)
unique_words = [word for word, count in word_counts.items() if count == 1]

print("Kata-kata unik (muncul sekali saja):")
print(unique_words[:2500])  # Tampilkan 100 kata unik pertama

# Jika ingin kata unik yang tidak termasuk stopwords:
from nltk.corpus import stopwords
stopwords_id = set(stopwords.words('indonesian'))
# Tambahkan stopwords tambahan jika perlu

unique_words_clean = [w for w in unique_words if w not in stopwords_id]
print("\nKata unik tanpa stopwords:")
print(unique_words_clean[:2500])

# Kamus slang
slangwords = {
    'tdk': 'tidak', 'sy': 'saya', 'bgt': 'banget', 'nn': 'nanti', 'ga': 'tidak',
    'gak': 'tidak', 'tak': 'tidak', 'trus': 'terus', 'dikit': 'sedikit', 'ni': 'ini',
    'aja': 'saja', 'hp': 'handphone', 'gmn': 'gimana', 'bgmn': 'bagaimana', 'kmn': 'kemana',
    'sih': 'si', 'sm': 'sama', 'bs': 'bisa', 'klo': 'kalau', 'kl': 'kalau', 'dr': 'dari',
    'dg': 'dengan', 'tp': 'tapi', 'blm': 'belum', 'udh': 'sudah', 'ud': 'sudah', 'lg': 'lagi',
    'skrg': 'sekarang', 'brp': 'berapa', 'eror': 'error', 'erorr': 'error', 'err': 'error',
    'bgus': 'bagus', 'bgs': 'bagus', 'lemot': 'lambat', 'lambt': 'lambat', 'aplk': 'aplikasi',
    'apk': 'aplikasi', 'app': 'aplikasi', 'kzl': 'kesal', 'jgn': 'jangan', 'bkn': 'bukan',
    'gt': 'gitu', 'cb': 'coba', 'parah': 'sangat', 'jos': 'bagus', 'top': 'bagus',
    'mantab': 'mantap', 'ok': 'oke', 'oky': 'oke', 'sip': 'oke', 'tlg': 'tolong',
    'thx': 'terima kasih', 'tx': 'terima kasih', 'pls': 'tolong', 'plis': 'tolong',
    'bgtz': 'banget', 'bener': 'benar', 'bner': 'benar', 'tb': 'tiba', 'tba': 'tiba',
    'gk': 'tidak', 'nggak': 'tidak', 'ngk': 'tidak', 'gajelas': 'tidak jelas',
    'ngebuk': 'bug', 'ngestuk': 'stuck', 'nyetuck': 'stuck', 'ngadat': 'macet',
    'tt': 'tiktok', 'anj': 'anjing', 'euy': 'euy', 'smpa': 'sumpah', 'udah': 'sudah',
    'gbs': 'tidak bisa', 'gabisa': 'tidak bisa', 'ngga': 'tidak', 'kpn': 'kapan',
    'msk': 'masuk', 'cm': 'cuma', 'bbrp': 'beberapa', 'lgsg': 'langsung', 'smp': 'sampai',
    'mskpn': 'meskipun', 'krn': 'karena', 'kdg': 'kadang', 'kdng': 'kadang', 'skli': 'sekali',
    'smpe': 'sampai', 'ny': 'nya', 'sbnrnya': 'sebenarnya', 'slalu': 'selalu', 'smua': 'semua',
    'stlh': 'setelah', 'kcl': 'kecil', 'bnyk': 'banyak', 'trsprh': 'terus terang', 'mskpn': 'meskipun',
    'benerin': 'perbaiki', 'pls': 'tolong', 'pliss': 'tolong', 'sblm': 'sebelum',
    'kli': 'kali', 'bbrapa': 'beberapa', 'lg': 'lagi', 'msk': 'masuk', 'km': 'kamu'
}

def fix_slangwords(text):
    words = text.split()
    fixed_words = []

    for word in words:
        if word.lower() in slangwords:
            fixed_words.append(slangwords[word.lower()])
        else:
            fixed_words.append(word)

    fixed_text = ' '.join(fixed_words)
    return fixed_text

df.head(10)

# Menampilkan dataset yang belum dibersihkan
df.head(10)

# Create a copy of the original DataFrame to avoid modifying the original data
clean_df = df.copy()

# Membersihkan teks dan menyimpannya di kolom 'text_clean'
clean_df['text_clean'] = clean_df['content'].apply(cleaningText)

# Mengubah huruf dalam teks menjadi huruf kecil dan menyimpannya di 'text_casefoldingText'
clean_df['text_casefoldingText'] = clean_df['text_clean'].apply(casefoldingText)

# Mengganti kata-kata slang dengan kata-kata standar dan menyimpannya di 'text_slangwords'
clean_df['text_slangwords'] = clean_df['text_casefoldingText'].apply(fix_slangwords)

# Memecah teks menjadi token (kata-kata) dan menyimpannya di 'text_tokenizingText'
clean_df['text_tokenizingText'] = clean_df['text_slangwords'].apply(tokenizingText)

# Menghapus kata-kata stop (kata-kata umum) dan menyimpannya di 'text_stopword'
clean_df['text_stopword'] = clean_df['text_tokenizingText'].apply(filteringText)

# Menggabungkan token-token menjadi kalimat dan menyimpannya di 'text_akhir'
clean_df['text_akhir'] = clean_df['text_stopword'].apply(toSentence)

clean_df.head()

# Loads positive lexicon data from GitHub
# Membaca data kamus kata-kata positif dari GitHub
lexicon_positive = dict()

response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')
reader = csv.reader(StringIO(response.text), delimiter=',')
for row in reader:
    lexicon_positive[row[0]] = int(row[1])


# Loads negative lexicon data from GitHub
# Membaca data kamus kata-kata negatif dari GitHub
lexicon_negative = dict()

response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')
reader = csv.reader(StringIO(response.text), delimiter=',')
for row in reader:
    lexicon_negative[row[0]] = int(row[1])

# Fungsi untuk menentukan polaritas sentimen dari tweet

def sentiment_analysis_lexicon_indonesia(text):
    # untuk setiap kata dalam teks:

    score = 0
    # Inisialisasi skor sentimen ke 0

    for word in text:
        # Mengulangi setiap kata dalam teks

        if (word in lexicon_positive):
            score = score + lexicon_positive[word]
            # Jika kata ada dalam kamus positif, tambahkan skornya ke skor sentimen

    for word in text:
        # Mengulangi setiap kata dalam teks (sekali lagi)

        if (word in lexicon_negative):
            score = score + lexicon_negative[word]
            # Jika kata ada dalam kamus negatif, kurangkan skornya dari skor sentimen

    polarity=''
    # Inisialisasi variabel polaritas

    if (score >= 0):
        polarity = 'positive'
        # Jika skor sentimen lebih besar atau sama dengan 0, maka polaritas adalah positif
    elif (score < 0):
        polarity = 'negative'
        # Jika skor sentimen kurang dari 0, maka polaritas adalah negatif
    else:
        polarity = 'neutral'
    # Ini adalah bagian yang bisa digunakan untuk menentukan polaritas netral jika diperlukan

    return score, polarity
    # Mengembalikan skor sentimen dan polaritas teks

results = clean_df['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)
results = list(zip(*results))
clean_df['polarity_score'] = results[0]
clean_df['polarity'] = results[1]
print(clean_df['polarity'].value_counts())

# Menghitung jumlah tiap polaritas
polarity_counts = clean_df['polarity'].value_counts()

# Plot diagram batang
plt.figure(figsize=(10, 6))
plt.pie(polarity_counts.values, labels=polarity_counts.index, autopct='%1.1f%%', colors=['red','skyblue'])
plt.title('Sentiment Polarity Count on Mobile TIX ID Review')
plt.show()

def sentiment_analysis_lexicon_indonesia(text):
    score = 0

    for word in text:
        if word in lexicon_positive:
            score += lexicon_positive[word]

    for word in text:
        if word in lexicon_negative:
            score += lexicon_negative[word]

    if score > 0:
        polarity = 'positive'
    elif score < 0:
        polarity = 'negative'
    else:
        polarity = 'neutral'

    return score, polarity

# Fungsi test
def test_sentiment_analysis():
    tweets = [
        "Aplikasi ini bagus sekali",
        "Jelek sekali ini aplikasi",
        "biasa saja",
        "jelek nih aplikasi",
        "Aplikasi ini bagus sekali"
    ]

    for tweet in tweets:
        words = tweet.lower().split()
        score, polarity = sentiment_analysis_lexicon_indonesia(words)
        print(f"Tweet: {tweet}")
        print(f"Score: {score}, Polarity: {polarity}")
        print("-" * 40)

# Jalankan test
test_sentiment_analysis()

#Mengatur opsi tampilan Pandas agar kolom dapat menampilkan teks hingga 3000 karakter.
pd.set_option('display.max_colwidth', 3000)

# Membuat DataFrame baru 'positive_tweets' yang hanya berisi tweet dengan polaritas positif.
positive_tweets = clean_df[clean_df['polarity'] == 'positive']

# Memilih hanya kolom-kolom tertentu dari DataFrame 'positive_tweets'.
positive_tweets = positive_tweets[['text_akhir', 'polarity_score', 'polarity','text_stopword']]

# Mengurutkan DataFrame 'positive_tweets' berdasarkan 'polarity_score' secara menurun.
positive_tweets = positive_tweets.sort_values(by='polarity_score', ascending=False)

# Mengatur ulang indeks DataFrame agar dimulai dari 0.
positive_tweets = positive_tweets.reset_index(drop=True)

# Menambahkan 1 ke semua indeks DataFrame.
positive_tweets.index += 1

# Mengatur opsi tampilan Pandas agar kolom dapat menampilkan teks hingga 3000 karakter.
pd.set_option('display.max_colwidth', 3000)

# Membuat DataFrame baru 'negative_tweets' yang hanya berisi tweet dengan polaritas negatif.
negative_tweets = clean_df[clean_df['polarity'] == 'negative']

# Memilih hanya kolom-kolom tertentu dari DataFrame 'negative_tweets'.
negative_tweets = negative_tweets[['text_akhir', 'polarity_score', 'polarity','text_stopword']]

# Mengurutkan DataFrame 'negative_tweets' berdasarkan 'polarity_score' secara menaik (ascending).
negative_tweets = negative_tweets.sort_values(by='polarity_score', ascending=True)

# Memilih 10 baris pertama dari DataFrame yang sudah diurutkan.
negative_tweets = negative_tweets[0:10]

# Mengatur ulang indeks DataFrame agar dimulai dari 0.
negative_tweets = negative_tweets.reset_index(drop=True)

# Menambahkan 1 ke semua indeks DataFrame.
negative_tweets.index += 1

# Membuat string kosong 'list_words' yang akan digunakan untuk mengumpulkan semua kata dari teks yang sudah dibersihkan.
list_words = ''

# Iterasi melalui setiap tweet dalam kolom 'text_stopword' dari DataFrame 'clean_df'.
for tweet in clean_df['text_stopword']:
    # Iterasi melalui setiap kata dalam tweet.
    for word in tweet:
        # Menambahkan kata ke dalam 'list_words'.
        list_words += ' ' + (word)

# Membuat objek WordCloud dengan parameter tertentu.
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(list_words)

fig, ax = plt.subplots(figsize=(8, 6)) # Membuat gambar dan sumbu untuk menampilkan word cloud
ax.set_title('Word Cloud of Tweets Data', fontsize=18) # Menetapkan judul untuk word cloud
ax.grid(False) # Menonaktifkan grid pada sumbu
ax.imshow((wordcloud)) # Menampilkan word cloud dalam gambar
fig.tight_layout(pad=0) # Mengatur layout gambar
ax.axis('off') # Menyembunyikan sumbu

# Menampilkan word cloud
plt.show()

# Membuat string kosong 'list_words' yang akan digunakan untuk mengumpulkan semua kata dari teks yang sudah dibersihkan dalam tweet positif.
list_words = ''

# Iterasi melalui setiap tweet dalam kolom 'text_stopword' dari DataFrame 'positive_tweets'.
for tweet in positive_tweets['text_stopword']:
    # Iterasi melalui setiap kata dalam tweet.
    for word in tweet:
        # Menambahkan kata ke dalam 'list_words'.
        list_words += ' ' + (word)

# Membuat objek WordCloud dengan parameter tertentu.
wordcloud = WordCloud(width=600, height=400, background_color='white', min_font_size=10).generate(list_words)

# Membuat gambar dan sumbu untuk menampilkan word cloud.
fig, ax = plt.subplots(figsize=(8, 6))

# Menetapkan judul untuk word cloud.
ax.set_title('Word Cloud of Positive Tweets Data', fontsize=18)

# Menonaktifkan grid pada sumbu.
ax.grid(False)

# Menampilkan word cloud dalam gambar.
ax.imshow((wordcloud))

# Mengatur layout gambar.
fig.tight_layout(pad=0)

# Menyembunyikan sumbu.
ax.axis('off')

# Menampilkan word cloud.
plt.show()

# Membuat string kosong 'list_words' yang akan digunakan untuk mengumpulkan semua kata dari teks yang sudah dibersihkan dalam tweet negatif.
list_words = ''

# Iterasi melalui setiap tweet dalam kolom 'text_stopword' dari DataFrame 'negative_tweets'.
for tweet in negative_tweets['text_stopword']:
    # Iterasi melalui setiap kata dalam tweet.
    for word in tweet:
        # Menambahkan kata ke dalam 'list_words'.
        list_words += ' ' + (word)

# Membuat objek WordCloud dengan parameter tertentu.
wordcloud = WordCloud(width=600, height=400, background_color='white', min_font_size=10).generate(list_words)

# Membuat gambar dan sumbu untuk menampilkan word cloud.
fig, ax = plt.subplots(figsize=(8, 6))

# Menetapkan judul untuk word cloud.
ax.set_title('Word Cloud of Negative Tweets Data', fontsize=18)

# Menonaktifkan grid pada sumbu.
ax.grid(False)

# Menampilkan word cloud dalam gambar.
ax.imshow((wordcloud))

# Mengatur layout gambar.
fig.tight_layout(pad=0)

# Menyembunyikan sumbu.
ax.axis('off')

# Menampilkan word cloud.
plt.show()

# Atur ukuran figure
plt.figure(figsize=(10, 6))

# Membuat distribusi panjang teks
clean_df['text_length'] = clean_df['text_akhir'].apply(lambda x: len(x.split()))

# Visualisasi distribusi panjang teks
sns.histplot(clean_df['text_length'], bins=50, kde=True, color='dodgerblue')

# Tambahkan judul dan label sumbu
plt.xlabel('Length Text', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Text Length Distribution', fontsize=14, fontweight='bold')

# Tampilkan plot
plt.show()

# Set ukuran figure
plt.figure(figsize=(12, 6))

# Visualisasi kata-kata paling sering muncul
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(clean_df['text_akhir'])
tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())

# Hitung frekuensi setiap kata
tfidf_df = tfidf_df.sum().reset_index(name='jumlah')
tfidf_df.rename(columns={'index': 'kata'}, inplace=True)

# Sort berdasarkan jumlah kemunculan (frekuensi)
tfidf_df = tfidf_df.sort_values('jumlah', ascending=False).head(20)

# Menambahkan kontras warna berdasarkan frekuensi (gradient)
colors = sns.color_palette("coolwarm", len(tfidf_df))

# Membuat barplot dengan kontras warna
sns.barplot(x='jumlah', y='kata', data=tfidf_df, palette=colors, hue='kata', dodge=False)

# Mengatur legenda agar tidak tampil jika tidak dibutuhkan
plt.legend([], [], frameon=False)

# Menambahkan judul dan label sumbu
plt.title('Most Frequent Words', fontsize=14, fontweight='bold')
plt.xlabel('Jumlah Kemunculan', fontsize=12)
plt.ylabel('Kata', fontsize=12)

# Tampilkan plot
plt.show()

# Ekstrak teks dan labels dari cleaned dataframe
texts = clean_df['text_akhir'].tolist()
labels = clean_df['polarity'].tolist()

# Inisialisasi tokenizer dan urutkan
vocab_limit = 2500
text_tokenizer = Tokenizer(num_words=vocab_limit, split=' ')
text_tokenizer.fit_on_texts(texts)
sequences = text_tokenizer.texts_to_sequences(texts)
padded_inputs = pad_sequences(sequences)

# Encode labels ke format numerik
label_encoder = LabelEncoder()
numeric_labels = label_encoder.fit_transform(labels)

# Split data training dan test
X_train, X_test, y_train, y_test = train_test_split(padded_inputs, numeric_labels, test_size=0.2, random_state=42)

# Custom callback untuk menghentikan training jika akurasi validasi telah melebihi ambang batas/threshold
class CustomCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        if logs.get('val_accuracy') > 0.92:
            print(f"\nVal accuracy telah melampaui 0,92 pada epoch {epoch+1}. Pelatihan dihentikan.")
            self.model.stop_training = True

# Inisialisasi callback
early_stopping = CustomCallback()

# Membangun model LSTM
from tensorflow.keras.callbacks import ReduceLROnPlateau # Import ReduceLROnPlateau

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_limit, output_dim=128, input_length=padded_inputs.shape[1]),
    tf.keras.layers.SpatialDropout1D(0.4),
    tf.keras.layers.LSTM(128, dropout=0.4, recurrent_dropout=0.4, return_sequences=False),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(3, activation='softmax')
])

# Optimasi: Learning rate scheduler
lr_schedule = ReduceLROnPlateau(
    monitor='val_loss', factor=0.2, patience=3, verbose=1, min_lr=1e-5
)

# Compile model
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=Adam(learning_rate=0.0005),
    metrics=['accuracy']
)

# Training model dengan callback
history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=64,
    validation_data=(X_test, y_test),
    callbacks=[early_stopping, lr_schedule],
    verbose=1
)

# Evaluasi Model
print("\nEvaluasi Model LSTM:")

# Evaluasi pada data training
results_train_LSTM = model.evaluate(X_train, y_train, verbose=1)
print("Training Loss: %.2f" % results_train_LSTM[0])
print("Training Accuracy: %.2f" % results_train_LSTM[1])

# Evaluasi pada data testing (validation)
results_val_LSTM = model.evaluate(X_test, y_test, verbose=1)
print("Validation Loss: %.2f" % results_val_LSTM[0])
print("Validation Accuracy: %.2f" % results_val_LSTM[1])

# Menyimpah hasil lstm
model.save("model_lstm.h5")

"""##**MODEL RNN**"""

X_train, X_test, y_train, y_test = train_test_split(padded_inputs, numeric_labels, test_size=0.3, random_state=42)

# Arsitektur Model RNN
model_rnn = Sequential()

# Embedding Layer
model_rnn.add(Embedding(input_dim=vocab_limit, output_dim=128, input_length=padded_inputs.shape[1]))

# Multiple RNN Layers
model_rnn.add(SimpleRNN(units=128, dropout=0.3, return_sequences=True))

# Simple RNN Layer
model_rnn.add(SimpleRNN(units=128, dropout=0.3, return_sequences=False))

# Fully Connected Layers
model_rnn.add(Dense(128, activation='relu'))
model_rnn.add(Dropout(0.3))
model_rnn.add(Dense(64, activation='relu'))
model_rnn.add(Dropout(0.3))

# Output Layer
model_rnn.add(Dense(len(set(numeric_labels)), activation='softmax'))

# Compile Model
model_rnn.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=['accuracy']
)

# Optimasi: Learning Rate Scheduler
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=1, min_lr=1e-5)

# Latih Model
history_rnn = model_rnn.fit(
    X_train, y_train,
    epochs=25,
    batch_size=128,
    validation_data=(X_test, y_test),
    callbacks=[early_stopping, lr_scheduler],
    verbose=1
)

# Evaluasi Model
print("\nEvaluasi Model RNN:")

# Evaluasi pada data training
results_rnn_train = model_rnn.evaluate(X_train, y_train, verbose=1)
print(f"Training Loss: {results_rnn_train[0]:.2f}")
print(f"Training Accuracy: {results_rnn_train[1]:.2f}")

# Evaluasi pada data testing (validation)
results_rnn_val = model_rnn.evaluate(X_test, y_test, verbose=1)
print(f"Validation Loss: {results_rnn_val[0]:.2f}")
print(f"Validation Accuracy: {results_rnn_val[1]:.2f}")

# Menyimpah hasil lstm
model.save("models_rrn.h5") # Changed the extension from .hhas to .h5

"""##**Model 3: CNN (70/30 Split)**"""

# Split data menjadi training dan testing
X_train, X_test, y_train, y_test = train_test_split(padded_inputs, numeric_labels, test_size=0.3, random_state=42)

# Arsitektur Model CNN
model_cnn = Sequential()

# Embedding Layer
model_cnn.add(Embedding(input_dim=vocab_limit, output_dim=128, input_length=padded_inputs.shape[1]))

# Multipel Conv1D Layer
model_cnn.add(Conv1D(filters=128, kernel_size=3, activation='relu'))
model_cnn.add(MaxPooling1D(pool_size=2))
model_cnn.add(Flatten())

# Fully Connected Layer dengan Dropout
model_cnn.add(Dense(128, activation='relu'))
model_cnn.add(Dropout(0.3))
model_cnn.add(Dense(64, activation='relu'))
model_cnn.add(Dropout(0.5))

# Output Layer untuk klasifikasi multi-kelas
model_cnn.add(Dense(len(set(numeric_labels)), activation='softmax'))

# Optimasi: learning rate scheduler
lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss', factor=0.2, patience=3, verbose=1, min_lr=1e-5
)

# Compile Model
model_cnn.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=['accuracy']
)

# Latih Model
history_cnn = model_cnn.fit(
    X_train, y_train,
    epochs=25,
    batch_size=128,
    validation_data=(X_test, y_test),
    callbacks=[early_stopping, lr_scheduler],
    verbose=1
)

# Evaluasi Model
print("\nEvaluasi Model CNN:")

# Evaluasi pada data training
results_cnn_train = model_cnn.evaluate(X_train, y_train, verbose=1)
print(f"Training Loss: {results_cnn_train[0]:.2f}")
print(f"Training Accuracy: {results_cnn_train[1]:.2f}")

# Evaluasi pada data testing (validation)
results_cnn_val = model_cnn.evaluate(X_test, y_test, verbose=1)
print(f"Validation Loss: {results_cnn_val[0]:.2f}")
print(f"Validation Accuracy: {results_cnn_val[1]:.2f}")

# Menyimpah hasil lstm
model.save("models_cnn.h5") # Changed the extension from .hhas to .h5

"""##**Model 4: GRU (80/20 Split)**"""

from tensorflow.keras.layers import BatchNormalization # Add this line
# Split data menjadi training dan testing
X_train, X_test, y_train, y_test = train_test_split(padded_inputs, numeric_labels, test_size=0.2, random_state=42)

# Arsitektur Model GRU
model_gru = Sequential()

# Embedding Layer
model_gru.add(Embedding(input_dim=vocab_limit, output_dim=256, input_length=padded_inputs.shape[1]))

# Multiple GRU Layers
model_gru.add(GRU(units=256, dropout=0.3, recurrent_dropout=0.3, return_sequences=False))

# Fully Connected Layers
model_gru.add(Dense(128, activation='relu'))
model_gru.add(BatchNormalization())
model_gru.add(Dropout(0.3))
model_gru.add(Dense(64, activation='relu'))
model_gru.add(Dropout(0.3))

# Output Layer Softmax
model_gru.add(Dense(len(set(numeric_labels)), activation='softmax'))

# Compile Model
model_gru.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=['accuracy']
)

# Optimasi: Learning Rate Scheduler
lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss', factor=0.2, patience=2, verbose=1, min_lr=1e-5
)

# Pelatihan Model
history_gru = model_gru.fit(
    X_train, y_train,
    epochs=15,
    batch_size=128,
    validation_data=(X_test, y_test),
    callbacks=[early_stopping, lr_scheduler],
    verbose=1
)

# Evaluasi Model
print("\nEvaluasi Model GRU:")

# Evaluasi pada data training
results_gru_train = model_gru.evaluate(X_train, y_train, verbose=1)
print(f"Training Loss: {results_gru_train[0]:.2f}")
print(f"Training Accuracy: {results_gru_train[1]:.2f}")

# Evaluasi pada data testing (validation)
results_gru_val = model_gru.evaluate(X_test, y_test, verbose=1)
print(f"Validation Loss: {results_gru_val[0]:.2f}")
print(f"Validation Accuracy: {results_gru_val[1]:.2f}")

model.save("model_gru.h5")

"""##**Evaluasi Model**"""

# Dictionary hasil evaluasi
models_results = {
    "LSTM": [results_train_LSTM[1], results_val_LSTM[1]],
    "RNN": [results_rnn_train[1], results_rnn_val[1]],
    "CNN": [results_cnn_train[1], results_cnn_val[1]],
    "GRU": [results_gru_train[1], results_gru_val[1]]
}

# Membuat DataFrame
df_accuracy = pd.DataFrame.from_dict(models_results, orient='index', columns=["Training Accuracy", "Validation Accuracy"])

# Menampilkan DataFrame
df_accuracy

"""# **Prediksi**"""

# Memuat model lstm
model = tf.keras.models.load_model('model_gru.h5')

# Input teks baru
new_texts = ["aplikasi tolol banyak bug nya bangsat"]
# Tokenisasi dengan tokenizer yang sudah digunakan saat pelatihan
new_sequences = text_tokenizer.texts_to_sequences(new_texts)
new_padded_inputs = pad_sequences(new_sequences, maxlen=padded_inputs.shape[1])

# Prediksi menggunakan model yang telah dimuat
predictions_LSTM = model.predict(new_padded_inputs)

# Konversi ke kelas kategorikal
categorical_class = ["negatif", "positif", "netral"]
predicted_labels_LSTM = categorical_class[np.argmax(predictions_LSTM, axis=1)[0]]

# Menampilkan hasil prediksi
print(f"\nKalimat: {new_texts[0]}")
print(f"Sentimen: {predicted_labels_LSTM}")